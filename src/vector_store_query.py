from langchain_chroma import Chroma
from langdetect import detect, DetectorFactory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from .loader_pdf_embedding import *
import os
from .utils import get_resource_path

CHROMA_DB_DIR = get_resource_path("./multi_lang_chroma_db")  # Chromaå‘é‡åº“å­˜å‚¨è·¯å¾„
CHUNK_SIZE = 512  # æ–‡æœ¬åˆ†å—å¤§å°
CHUNK_OVERLAP = 64  # åˆ†å—é‡å é•¿åº¦
DetectorFactory.seed = 0  # å›ºå®šè¯­è¨€æ£€æµ‹ç§å­ï¼Œç»“æœç¨³å®š


def is_file_in_chroma_db(db, file_path):
    """
    æ£€æŸ¥æŒ‡å®šæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºChromaå‘é‡åº“ä¸­
    å‚æ•°ï¼š
        db: Chromaæ•°æ®åº“å®ä¾‹
        file_path: å¾…æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ "./è®ºæ–‡1.pdf"ï¼‰
    è¿”å›ï¼š
        bool: Trueï¼ˆå·²å­˜åœ¨ï¼‰/Falseï¼ˆä¸å­˜åœ¨ï¼‰
    """
    # æå–æ–‡ä»¶åï¼ˆå’ŒåŸå‡½æ•°ä¸­sourceå…ƒæ•°æ®ä¿æŒå®Œå…¨ä¸€è‡´ï¼‰
    file_name = os.path.basename(file_path)

    try:
        # æ ¸å¿ƒï¼šé€šè¿‡å…ƒæ•°æ®è¿‡æ»¤æŸ¥è¯¢è¯¥æ–‡ä»¶çš„æ‰€æœ‰è®°å½•
        # whereå‚æ•°å®ç°ç²¾å‡†åŒ¹é…sourceå­—æ®µï¼ˆå­˜å‚¨çš„æ˜¯æ–‡ä»¶åï¼‰
        query_results = db.get(
            where={"source": file_name}  # åŒ¹é…åŸå‡½æ•°æ·»åŠ çš„sourceå…ƒæ•°æ®
        )

        # å¦‚æœæŸ¥è¯¢ç»“æœä¸­æœ‰idï¼Œè¯´æ˜è¯¥æ–‡ä»¶å·²å­˜åœ¨
        has_records = len(query_results["ids"]) > 0
        if has_records:
            print(f"â„¹ï¸ æ–‡ä»¶ {file_name} å·²å­˜åœ¨äºå‘é‡åº“ä¸­ï¼ˆå…± {len(query_results['ids'])} ä¸ªæ–‡æœ¬å—ï¼‰")
        return has_records

    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æ–‡ä»¶ {file_name} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™ï¼š{str(e)}")
        return False


def build_multi_lang_chroma_db(doc_paths):
    """
    æ‰¹é‡å¤„ç†å¤šè¯­è¨€è®ºæ–‡ï¼ˆæ–°å¢é‡å¤æ£€æŸ¥é€»è¾‘ï¼‰ï¼š
    1. é€ä¸ªæ£€æµ‹è®ºæ–‡è¯­è¨€ â†’ å¯¹åº”æ¨¡å‹ç¼–ç 
    2. ä¸ºæ–‡æ¡£æ·»åŠ è¯­è¨€å…ƒæ•°æ®ï¼ˆlang: zh/enï¼‰
    3. åˆå¹¶æ‰€æœ‰å‘é‡åˆ°åŒä¸€ä¸ªChromaåº“
    4. å‰ç½®æ£€æŸ¥ï¼šè·³è¿‡å·²å­˜å…¥çš„æ–‡ä»¶
    """
    # åˆå§‹åŒ–ç©ºçš„Chromaåº“ï¼ˆç»Ÿä¸€å­˜å‚¨ï¼‰
    db = Chroma(
        embedding_function=get_bge_embeddings("zh"),
        persist_directory=CHROMA_DB_DIR
    )
    try:
        for file_path in doc_paths:
            # ===== æ–°å¢ï¼šå‰ç½®æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ =====
            if is_file_in_chroma_db(db, file_path):
                print(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼š{os.path.basename(file_path)}")
                continue
            # ======================================

            if not os.path.exists(file_path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
                continue

            # æ­¥éª¤1ï¼šæ£€æµ‹å½“å‰è®ºæ–‡è¯­è¨€
            lang = detect_document_language(file_path)
            if lang == "unknown":
                print(f"âš ï¸ æ— æ³•æ£€æµ‹{file_path}è¯­è¨€ï¼Œä½¿ç”¨è·¨è¯­è¨€æ¨¡å‹")

            # æ­¥éª¤2ï¼šåŠ è½½å¯¹åº”æ¨¡å‹
            embeddings = get_bge_embeddings(lang)

            # æ­¥éª¤3ï¼šè®ºæ–‡åŠ è½½+è¿‡æ»¤+åˆ†å—ï¼ˆå­¦æœ¯PDFä¼˜åŒ–ï¼‰
            loader = PyMuPDFLoader(file_path) if file_path.endswith(".pdf") else TextLoader(file_path, encoding="utf-8")
            docs = loader.load()
            # è¿‡æ»¤æ— æ•ˆæ–‡æœ¬ï¼ˆé¡µçœ‰é¡µè„šã€ä¹±ç ï¼‰
            filtered_docs = []
            for doc in docs:
                content = doc.page_content.strip()
                if len(content) > 20 and "ï¿½ï¿½" not in content:
                    filtered_docs.append(doc)
            # å­¦æœ¯åˆ†å—
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "ã€", " ", "$", "##", ",", ".", ]
            )
            split_docs = text_splitter.split_documents(filtered_docs)

            # æ­¥éª¤4ï¼šæ·»åŠ å…ƒæ•°æ®ï¼ˆè¯­è¨€+æ–‡ä»¶è·¯å¾„ï¼‰ï¼Œå…³é”®ï¼ç”¨äºæ£€ç´¢è¿‡æ»¤
            for doc in split_docs:
                doc.metadata["lang"] = lang  # è¯­è¨€å…ƒæ•°æ®
                doc.metadata["source"] = os.path.basename(file_path)  # æ¥æºè®ºæ–‡åç§°

            # æ­¥éª¤5ï¼šå°†å½“å‰è®ºæ–‡çš„å‘é‡æ·»åŠ åˆ°ç»Ÿä¸€Chromaåº“
            db.add_documents(documents=split_docs, embedding=embeddings)
            print(f"âœ… æˆåŠŸæ·»åŠ è®ºæ–‡ï¼š{os.path.basename(file_path)} | è¯­è¨€ï¼š{lang} | æ–‡æœ¬å—æ•°ï¼š{len(split_docs)}")

        print(f"\nğŸ‰ æ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæˆï¼å‘é‡åº“å­˜å‚¨è·¯å¾„ï¼š{CHROMA_DB_DIR}")
        return db
    except Exception as e:
        print("è¾“å…¥pdfæˆ–txtæ ¼å¼æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥pdfæ˜¯å¦å±äºæ‰«æå›¾ç‰‡")


# å¤šè¯­è¨€RAGæ£€ç´¢å‡½æ•°ï¼ˆæ ¸å¿ƒï¼šæŸ¥è¯¢è¯­è¨€åŒ¹é…+å…ƒæ•°æ®è¿‡æ»¤ï¼‰
def multi_lang_rag_search(query, db):
    """
    å¤šè¯­è¨€æ£€ç´¢é€»è¾‘ï¼š
    1. æ£€æµ‹æŸ¥è¯¢è¯­è¨€ â†’ ç”¨å¯¹åº”æ¨¡å‹ç”ŸæˆæŸ¥è¯¢å‘é‡
    2. è¿‡æ»¤åŒè¯­è¨€çš„è®ºæ–‡ç‰‡æ®µ â†’ ç²¾å‡†æ£€ç´¢
    3. æ”¯æŒè·¨è®ºæ–‡è”åˆæ£€ç´¢
    """
    try:
        # æ­¥éª¤1ï¼šæ£€æµ‹æŸ¥è¯¢è¯­è¨€
        query_lang = detect_text_language(query)
        print(f"ğŸ” æ£€æµ‹åˆ°æŸ¥è¯¢è¯­è¨€ï¼š{query_lang}")

        # æ­¥éª¤2ï¼šè·å–å¯¹åº”æ¨¡å‹ï¼Œç”ŸæˆæŸ¥è¯¢å‘é‡
        embeddings = get_bge_embeddings(query_lang)

        # æ­¥éª¤3ï¼šæ„å»ºå¸¦è¯­è¨€è¿‡æ»¤çš„æ£€ç´¢å™¨
        retriever = db.as_retriever(
            search_kwargs={
                "k": 3,
                "filter": {"lang": query_lang}  # åªæ£€ç´¢åŒè¯­è¨€ç‰‡æ®µ
            },
            embedding=embeddings
        )

        # æ­¥éª¤4ï¼šæ‰§è¡Œæ£€ç´¢ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šæ›¿æ¢æ—§æ–¹æ³•ï¼‰
        # é€‚é…LangChain v0.1+ æ–°ç‰ˆæ¥å£
        relevant_docs = retriever.invoke(query)

        # ç©ºç»“æœå¤„ç†
        if not relevant_docs:
            return f"âŒ æœªæ£€ç´¢åˆ°{query_lang}è¯­è¨€çš„ç›¸å…³å†…å®¹"

        # æ­¥éª¤5ï¼šç»“æ„åŒ–æ‹¼æ¥ç»“æœ
        result = []
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source", "æœªçŸ¥è®ºæ–‡")
            result.append(f"ã€ç›¸å…³ç‰‡æ®µ{i + 1} | æ¥æºï¼š{source}ã€‘\n{doc.page_content}")
        return "\n\n".join(result)

    # å…œåº•ï¼šé€‚é…ææ—§ç‰ˆæœ¬LangChainï¼ˆå…¼å®¹get_relevant_documentsï¼‰
    except AttributeError as e:
        if "invoke" in str(e):
            try:
                relevant_docs = retriever.get_relevant_documents(query)
                if not relevant_docs:
                    return f"âŒ æœªæ£€ç´¢åˆ°{query_lang}è¯­è¨€çš„ç›¸å…³å†…å®¹"
                result = []
                for i, doc in enumerate(relevant_docs):
                    source = doc.metadata.get("source", "æœªçŸ¥è®ºæ–‡")
                    result.append(f"ã€ç›¸å…³ç‰‡æ®µ{i + 1} | æ¥æºï¼š{source}ã€‘\n{doc.page_content}")
                return "\n\n".join(result)
            except Exception as e2:
                return f"âŒ æ£€ç´¢æ–¹æ³•é€‚é…å¤±è´¥ï¼š{str(e2)}"
        return f"âŒ å±æ€§é”™è¯¯ï¼š{str(e)}"

    # æ•è·å…¶ä»–å¼‚å¸¸ï¼ˆæ¨¡å‹åŠ è½½ã€å‘é‡åº“è¿æ¥ç­‰ï¼‰
    except Exception as e:
        return f"âŒ æ£€ç´¢å‡ºé”™ï¼š{str(e)}"


